{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para que o Jupyter consiga carregar o Spark corretamente no notebook\n",
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "\n",
    "# Para que os executors tenham mais memória e não falhem por falta de recursos\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--executor-memory 1G pyspark-shell'\n",
    "\n",
    "# A partir daqui é código Spark que normalmente é executado com um comando similar ao comando abaixo:\n",
    "# spark-submit --executor-memory 1G nome_do_script.py\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# A linha abaixo está comentada porque essa é a forma de executar Spark em uma instalação local usando todos os cores\n",
    "#conf = SparkConf().setMaster(\"local[*]\").setAppName(\"NomeDoApp\")\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "ratings = sc.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.data\")\n",
    "rating_values = ratings.map(lambda x: x.split()[2])\n",
    "result = rating_values.countByValue()\n",
    "\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for rating, number_of_ratings in sortedResults.items():\n",
    "    print(\"{} usuários colocaram a nota {}\".format(number_of_ratings, rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Value Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ratings_as_key(line):\n",
    "    fields = line.split()\n",
    "    rating_field = int(fields[2])\n",
    "    return (rating_field, 1)\n",
    "\n",
    "ratings = sc.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.data\")\n",
    "ratings_count = ratings.map(parse_ratings_as_key)\n",
    "ratings_sum = ratings_count.reduceByKey(lambda x, y: x + y)\n",
    "results = ratings_sum.collect()\n",
    "for rating, number_of_ratings in results:\n",
    "    print(\"{} usuários colocaram a nota {}\".format(number_of_ratings, rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_user_and_rating(line):\n",
    "    fields = line.split()\n",
    "    user_field = int(fields[0])\n",
    "    rating_field = int(fields[2])\n",
    "    return (user_field, rating_field)\n",
    "\n",
    "ratings = sc.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.data\")\n",
    "ratings_by_user = ratings.map(parse_user_and_rating)\n",
    "totals_by_user = ratings_by_user.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "ratings_avg = totals_by_user.mapValues(lambda x: x[0] / float(x[1]))\n",
    "results = ratings_avg.collect()\n",
    "for user, avg in results:\n",
    "    print(\"A média de notas do usuário {} é {}\".format(user, avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_and_rating(line):\n",
    "    fields = line.split()\n",
    "    movie_field = fields[1]\n",
    "    rating_field = fields[2]\n",
    "    return (movie_field, rating_field)\n",
    "\n",
    "ratings = sc.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.data\")\n",
    "ratings_by_movie = ratings.map(parse_movie_and_rating)\n",
    "star_wars_ratings = ratings_by_movie.filter(lambda x: \"50\" == x[0])\n",
    "min_start_wars_rating = star_wars_ratings.reduceByKey(lambda x, y: min(x, y))\n",
    "results = min_start_wars_rating.collect()\n",
    "\n",
    "for movie, rating in results:\n",
    "    print(\"A pior nota do filme {} foi {}\".format(movie, rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# field type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_and_rating(line):\n",
    "    fields = line.split()\n",
    "    movie_field = int(fields[1])\n",
    "    rating_field = int(fields[2])\n",
    "    return (movie_field, rating_field)\n",
    "\n",
    "ratings = sc.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.data\")\n",
    "print(ratings.take(5))\n",
    "ratings_by_movie = ratings.map(parse_movie_and_rating)\n",
    "print(ratings_by_movie.take(5))\n",
    "star_wars_ratings = ratings_by_movie.filter(lambda x: 50 == x[0])\n",
    "min_start_wars_rating = star_wars_ratings.reduceByKey(lambda x, y: min(x, y))\n",
    "results = min_start_wars_rating.collect()\n",
    "\n",
    "for movie, rating in results:\n",
    "    print(\"A pior nota do filme {} foi {}\".format(movie, rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melhorando formatação do resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_and_rating(line):\n",
    "    fields = line.split()\n",
    "    movie_field = int(fields[1])\n",
    "    rating_field = int(fields[2])\n",
    "    return (movie_field, rating_field)\n",
    "\n",
    "movies = sc.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.item\").map(lambda x: x.split(\"|\"))\n",
    "movies_dict = movies.map(lambda x: (int(x[0]), x[1])).collectAsMap()\n",
    "ratings = sc.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.data\")\n",
    "ratings_by_movie = ratings.map(parse_movie_and_rating)\n",
    "star_wars_ratings = ratings_by_movie.filter(lambda x: 50 == x[0])\n",
    "min_start_wars_rating = star_wars_ratings.reduceByKey(lambda x, y: min(x, y))\n",
    "results = min_start_wars_rating.collect();\n",
    "print(results)\n",
    "\n",
    "for movie, rating in results:\n",
    "    print(\"A pior nota do filme {} foi {}\".format(movies_dict[movie], rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendendo melhor o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_and_rating(line):\n",
    "    fields = line.split()\n",
    "    movie_field = int(fields[1])\n",
    "    rating_field = int(fields[2])\n",
    "    return (movie_field, rating_field)\n",
    "\n",
    "movies = sc.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.item\").map(lambda x: x.split(\"|\"))\n",
    "movies_dict = movies.map(lambda x: (int(x[0]), x[1])).collectAsMap()\n",
    "ratings = sc.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.data\")\n",
    "ratings_by_movie = ratings.map(parse_movie_and_rating)\n",
    "star_wars_ratings = ratings_by_movie.filter(lambda x: 1536 == x[0]) # Tente com o filme 1536\n",
    "min_start_wars_rating = star_wars_ratings.reduceByKey(lambda x, y: min(x, y))\n",
    "results = min_start_wars_rating.collect()\n",
    "ratings_count = star_wars_ratings.countByKey()\n",
    "\n",
    "for movie, rating in results:\n",
    "    print(\"A pior nota do filme {} foi {} com {} notas\".format(movies_dict[movie], rating, ratings_count[movie]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"text.txt\", \"w\") as text_file:\n",
    "#     text_file.write(\"Um texto simples\\n\")\n",
    "#     text_file.write(\"Um texto de exemplo\\n\")\n",
    "#     text_file.write(\"Exemplo de texto\")\n",
    "\n",
    "# text = sc.textFile(\"file:///home/dataeng6/text.txt\") # Troque hadoop pelo seu login do JupyterHub\n",
    "# rdd = text.map(lambda x: x)\n",
    "# print(rdd.take(10))\n",
    "# rdd = text.map(lambda x: x.split())\n",
    "# print(rdd.take(10))\n",
    "# rdd = text.flatMap(lambda x: x.split()) # Experimente usar x.lower().split() para ver o que muda no resultado\n",
    "# print(rdd.take(10))\n",
    "\n",
    "# word_count = rdd.countByValue()\n",
    "# for word, count in word_count.items():\n",
    "#     print(\"{}: {}\".format(word, count))\n",
    "\n",
    "# with open(\"text.txt\", \"w\") as text_file:\n",
    "#     text_file.write(\"Um texto simples\\n\")\n",
    "#     text_file.write(\"Um texto de exemplo\\n\")\n",
    "#     text_file.write(\"Exemplo de texto\")\n",
    "\n",
    "text = sc.parallelize([\"Um texto simples\", \"Um texto de exemplo\", \"Exemplo de texto\"])\n",
    "\n",
    "# text = sc.textFile(\"file:///home/dataeng6/text.txt\") # Troque hadoop pelo seu login do JupyterHub\n",
    "# rdd = text.map(lambda x: x)\n",
    "# print(rdd.take(10))\n",
    "rdd = text.map(lambda x: x.split())\n",
    "print(rdd.take(10))\n",
    "rdd = text.flatMap(lambda x: x.split()) # Experimente usar x.lower().split() para ver o que muda no resultado\n",
    "print(rdd.take(10))\n",
    "\n",
    "# word_count = rdd.countByValue()\n",
    "# print('word_count: ', word_count)\n",
    "# for word, count in word_count.items():\n",
    "#     print(\"{}: {}\".format(word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sortByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"text.txt\", \"w\") as text_file:\n",
    "#     text_file.write(\"Um texto simples\\n\")\n",
    "#     text_file.write(\"Um texto de exemplo\\n\")\n",
    "#     text_file.write(\"Exemplo de texto\")\n",
    "\n",
    "# text = sc.textFile(\"file:///home/hadoop/text.txt\") # Troque hadoop pelo seu login do JupyterHub\n",
    "# rdd = text.flatMap(lambda x: x.lower().split())\n",
    "# rdd_counted = rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "# rdd_sorted = rdd_counted.map(lambda x: (x[1], x[0])).sortByKey() # Experimente adicionar o argumento ascending=False\n",
    "\n",
    "# word_count = rdd_sorted.collect()\n",
    "# for count, word in word_count:\n",
    "#     print(\"{}: {}\".format(count, word))\n",
    "\n",
    "# with open(\"text.txt\", \"w\") as text_file:\n",
    "#     text_file.write(\"Um texto simples\\n\")\n",
    "#     text_file.write(\"Um texto de exemplo\\n\")\n",
    "#     text_file.write(\"Exemplo de texto\")\n",
    "\n",
    "text = sc.parallelize([\"Um texto simples\", \"Um texto de exemplo\", \"Exemplo de texto\"])\n",
    "\n",
    "# text = sc.textFile(\"file:///home/hadoop/text.txt\") # Troque hadoop pelo seu login do JupyterHub\n",
    "rdd = text.flatMap(lambda x: x.lower().split())\n",
    "rdd_counted = rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "rdd_sorted = rdd_counted.map(lambda x: (x[1], x[0])).sortByKey(ascending=False) # Experimente adicionar o argumento ascending=False\n",
    "\n",
    "word_count = rdd_sorted.collect()\n",
    "for count, word in word_count:\n",
    "    print(\"{}: {}\".format(count, word))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_count(line):\n",
    "    fields = line.split()\n",
    "    movie_field = int(fields[1])\n",
    "    return (movie_field, 1)\n",
    "\n",
    "movies = sc.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.item\").map(lambda x: x.split(\"|\"))\n",
    "dict_movies_names = movies.map(lambda x: (int(x[0]), x[1])).collectAsMap()\n",
    "movies_dict = sc.broadcast(dict_movies_names) # Enviar dict para os executors apenas uma vez\n",
    "ratings = sc.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.data\")\n",
    "ratings_count_by_movie = ratings.map(parse_movie_count).reduceByKey(lambda x, y: x + y)\n",
    "ratings_count_by_movie = ratings_count_by_movie.map(lambda x: (x[1], x[0])).sortByKey(ascending=False)\n",
    "ratings_count_by_movie = ratings_count_by_movie.map(lambda x: (x[0], movies_dict.value[x[1]]))\n",
    "\n",
    "result = ratings_count_by_movie.collect()\n",
    "for count, movie in result:\n",
    "    print(\"{}: {}\".format(count, movie))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "def parse_ratings(line):\n",
    "    fields = line.split()\n",
    "    return Row(user_id=int(fields[0]), \n",
    "               movie_id=int(fields[1]), \n",
    "               rating=int(fields[2]), \n",
    "               timestamp=int(fields[3]))\n",
    "\n",
    "def parse_movies(line):\n",
    "    fields = line.split(\"|\")\n",
    "    return Row(movie_id=int(fields[0]), \n",
    "               name=fields[1])\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "data = spark.sparkContext.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.data\")\n",
    "ratings = data.map(parse_ratings)\n",
    "ratings_df = spark.createDataFrame(ratings).cache()\n",
    "ratings_df.createOrReplaceTempView(\"ratings\")\n",
    "\n",
    "data = spark.sparkContext.textFile(\"s3a://data-eng-t2-school/spark/ml-100k/u.item\")\n",
    "movies = data.map(parse_movies)\n",
    "movies_df = spark.createDataFrame(movies).cache()\n",
    "movies_df.createOrReplaceTempView(\"movies\")\n",
    "\n",
    "# AVISO: lembre de executar o comando spark.stop() no último bloco de código quando acabar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT * FROM ratings LIMIT 10\")\n",
    "\n",
    "for r in result.collect():\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filmes com mais notas registradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.groupBy(\"user_id\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filmes com mais notas registradas (melhorado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.join(movies_df, ratings_df.movie_id == movies_df.movie_id).groupBy(movies_df.name).count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados\n",
    "\n",
    "**Descrição das colunas:**  \n",
    "timestamp,user_id,action,adId,campaignId \n",
    "\n",
    "**Amostra:**  \n",
    "2016-09-21 22:11:00,7c74953c-66cc-48bd-9d02-a02bf039cf3f,click,adId_09,campaignId_01  \n",
    "2016-06-25 18:29:00,676a083e-2f8e-4ff2-9ec2-270f7f9d6033,view,adId_09,campaignId_02  \n",
    "2016-02-14 19:03:00,77158997-0dfa-48b7-9149-973dc151ef8d,click,adId_02,campaignId_02  \n",
    "2016-03-26 06:27:00,78aa2467-b502-413b-94e9-04ec8210bd13,click,adId_07,campaignId_03\n",
    "\n",
    "**Path:**  \n",
    "s3a://data-eng-t2-school/spark/ad_action.csv\n",
    "\n",
    "## Atividade 1\n",
    "\n",
    "Qual é o anúncio mais popular?\n",
    "\n",
    "**Resposta:**  \n",
    "adId_06\n",
    "\n",
    "## Atividade 2\n",
    "\n",
    "Quantos clicks gerou a campanha mais popular?\n",
    "\n",
    "**Resposta:**  \n",
    "63983 clicks\n",
    "\n",
    "## Atividade 3\n",
    "\n",
    "Algum usuário só visualizou? Quantas actions foram enviadas por usuário que só visualizou?\n",
    "\n",
    "**Resposta:**  \n",
    "Sim, o usuário d99871cb-98b7-4ac5-97a5-b9a26c0f897b enviou apenas 1 action\n",
    "\n",
    "## Atividade 4\n",
    "\n",
    "Dos 10411 usuários, quantos usuários clicam mais que visualizam?\n",
    "\n",
    "**Resposta:**  \n",
    "410\n",
    "\n",
    "## Atividade 5\n",
    "\n",
    "Calcule a quantidade de clicks por dia da semana e apresente o resultado em ordem decrescente de quantidade de clicks.\n",
    "\n",
    "**Resposta**  \n",
    "27918, 4  \n",
    "25424, 5  \n",
    "25028, 0  \n",
    "25027, 3  \n",
    "25020, 6  \n",
    "24973, 1  \n",
    "24915, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "def parse_ad_action(line):\n",
    "    fields = line.split(\",\")\n",
    "    return Row(timestamp=fields[0], \n",
    "               user_id=fields[1], \n",
    "               action=fields[2], \n",
    "               adId=fields[3],\n",
    "               campaignId=fields[4]\n",
    "              )\n",
    "\n",
    "# def parse_movies(line):\n",
    "#     fields = line.split(\"|\")\n",
    "#     return Row(movie_id=int(fields[0]), \n",
    "#                name=fields[1])\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "data = spark.sparkContext.textFile(\"s3a://data-eng-t2-school/spark/ad_action.csv\")\n",
    "ad_action = data.map(parse_ad_action)\n",
    "print(ad_action)\n",
    "ad_action_df = spark.createDataFrame(ad_action).cache()\n",
    "ad_action_df.createOrReplaceTempView(\"ad_action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_action_df.groupBy(\"adId\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_action_df2 = ad_action_df.filter(ad_action_df['action'] == \"click\")\n",
    "ad_action_df2.groupBy(\"campaignId\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_action_df3 = ad_action_df.filter(ad_action_df['action'] == \"view\")\n",
    "ad_action_df4 = ad_action_df.filter(ad_action_df['action'] == \"click\")\n",
    "\n",
    "ad_action_df3.join(ad_action_df4, ad_action_df4.user_id != ad_action_df3.user_id)\n",
    "\n",
    "# ad_action_df3.show()\n",
    "\n",
    "ad_action_df5 = ad_action_df3.filter(ad_action_df['action'] == \"view\")\n",
    "ad_action_df5.show()\n",
    "\n",
    "# ratings_df.join(movies_df, ratings_df.movie_id == movies_df.movie_id).groupBy(movies_df.name).count().orderBy(\"count\", ascending=False).show()\n",
    "# ad_action_df4.show(5)\n",
    "# ad_action_df3.groupBy(\"user_id\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
